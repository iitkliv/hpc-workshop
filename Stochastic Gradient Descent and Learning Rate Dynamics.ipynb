{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent and Learning Rate Dynamics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Packages\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data:\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "BatchSize = 100\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) # Creating dataloader\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three',\n",
    "           'four', 'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network:\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU())\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net1 = NeuralNet()\n",
    "net2 = NeuralNet()\n",
    "net3 = NeuralNet()\n",
    "net4 = NeuralNet()\n",
    "\n",
    "if use_gpu:\n",
    "    net1 = net1.cuda()\n",
    "    net2 = net2.cuda()\n",
    "    net3 = net3.cuda()\n",
    "    net4 = net4.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with different Optimizer:\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model,optimizer,IP,LB):\n",
    "    optimizer.zero_grad()\n",
    "    OP = model(IP)\n",
    "    loss = criterion(F.log_softmax(OP,dim=1), LB)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=1e-4)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=1e-4, momentum=0.9)\n",
    "optimizer3 = optim.Adagrad(net3.parameters(), lr=1e-4)\n",
    "optimizer4 = optim.Adam(net4.parameters(), lr=1e-4)\n",
    "PlotAcc1 = []\n",
    "PlotAcc2 = []\n",
    "PlotAcc3 = []\n",
    "PlotAcc4 = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    correct4 = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28)).cuda(), Variable(labels).cuda()\n",
    "            \n",
    "        _ = Train(net1,optimizer1,inputs,labels)\n",
    "        _ = Train(net2,optimizer2,inputs,labels)\n",
    "        _ = Train(net3,optimizer3,inputs,labels)\n",
    "        _ = Train(net4,optimizer4,inputs,labels)\n",
    "        \n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28)).cuda(), labels.cuda()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28)), labels\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        outputs = net1(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct1 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct2 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net3(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct3 += (predicted == labels).sum()\n",
    "        \n",
    "        outputs = net4(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct4 += (predicted == labels).sum()\n",
    "    if use_gpu:    \n",
    "        PlotAcc1.append(correct1.cpu().numpy()/float(total))\n",
    "        PlotAcc2.append(correct2.cpu().numpy()/float(total))\n",
    "        PlotAcc3.append(correct3.cpu().numpy()/float(total))\n",
    "        PlotAcc4.append(correct4.cpu().numpy()/float(total))\n",
    "    else:\n",
    "        PlotAcc1.append(correct1.numpy()/float(total))\n",
    "        PlotAcc2.append(correct2.numpy()/float(total))\n",
    "        PlotAcc3.append(correct3.numpy()/float(total))\n",
    "        PlotAcc4.append(correct4.numpy()/float(total))\n",
    "        \n",
    "    print('Epoch %d '%(epoch+1))\n",
    "print('Finished Training')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),PlotAcc1,'r-',label='SGD')\n",
    "plt.plot(range(epoch+1),PlotAcc2,'c-',label='SGD with momentum')\n",
    "plt.plot(range(epoch+1),PlotAcc3,'g-',label='Adagrad')\n",
    "plt.plot(range(epoch+1),PlotAcc4,'b-',label='Adam')        \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Testing Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network:\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU())\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10)            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net1 = NeuralNet()\n",
    "net2 = NeuralNet()\n",
    "net3 = NeuralNet()\n",
    "\n",
    "if use_gpu:\n",
    "    net1 = net1.cuda()\n",
    "    net2 = net2.cuda()\n",
    "    net3 = net3.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Learning Rate Dynamics:\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=1e-2)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=1e-2)\n",
    "optimizer3 = optim.SGD(net3.parameters(), lr=1e-2)\n",
    "\n",
    "scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer2, milestones=[3,7,9], gamma=0.9)\n",
    "scheduler3 = optim.lr_scheduler.ExponentialLR(optimizer3, gamma=0.99)\n",
    "\n",
    "Plotloss1 = []\n",
    "Plotloss2 = []\n",
    "Plotloss3 = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    \n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    loss3 = 0\n",
    "    scheduler2.step()\n",
    "    scheduler3.step()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28)).cuda(), Variable(labels).cuda()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs.view(-1, 28*28)), Variable(labels)\n",
    "            \n",
    "        loss1 += Train(net1,optimizer1,inputs,labels).item()\n",
    "        loss2 += Train(net2,optimizer2,inputs,labels).item()\n",
    "        loss3 += Train(net3,optimizer3,inputs,labels).item()   \n",
    "        \n",
    "    Plotloss1.append(loss1/(60000/BatchSize))\n",
    "    Plotloss2.append(loss2/(60000/BatchSize))\n",
    "    Plotloss3.append(loss3/(60000/BatchSize))\n",
    "    for opt in optimizer1.param_groups:\n",
    "        print('SGD Learning Rate: '+str(opt['lr']))\n",
    "    for opt in optimizer2.param_groups:\n",
    "        print('SGD (step decay) Learning Rate: '+str(opt['lr']))\n",
    "    for opt in optimizer3.param_groups:\n",
    "        print('SGD (step exp_decay) Learning Rate: '+str(opt['lr']))\n",
    "    print('Epoch %d ; SGD:  %f ; SGD step_decay:  %f ; SGD exp_decay:  %f'%((epoch+1),loss1/(60000/BatchSize),loss2/(60000/BatchSize),loss3/(60000/BatchSize)))\n",
    "    print('_______________________________________________________________')\n",
    "print('Finished Training')\n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),Plotloss1,'r-',label='SGD')\n",
    "plt.plot(range(epoch+1),Plotloss2,'g-',label='SGD with step_decay')     \n",
    "plt.plot(range(epoch+1),Plotloss3,'b-',label='SGD with exp_decay')  \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
